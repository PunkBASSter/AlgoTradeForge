# Backtest Debug & Event Export System — Requirements

## 1. Overview

A unified event export system that serves three consumers from a single event stream:

1. **Visual Debugger** — real-time FE client connected via WebSocket, interactive step-through
2. **AI Debugger (Claude Code)** — post-hoc analysis via JSONL files + SQLite index
3. **Trade Logger** — persistent trade/order records, streamed as JSONL during the run, persisted to SQLite after run completes

The system supports multiple execution modes with different export filters, and handles concurrent parallel runs by writing to separate output directories.

---

## 2. Event Model

### 2.1 Canonical Event Shape

All events share a minimal envelope with compact field names:

| Field     | Key  | Type     | Description                                    |
|-----------|------|----------|------------------------------------------------|
| Timestamp | `ts` | ISO 8601 | UTC time of emission                           |
| Sequence  | `sq` | uint64   | Monotonic per-run, guarantees ordering         |
| Type      | `_t` | string   | Event type identifier (see §2.2)               |
| Source    | `src`| string   | Emitting component/subsystem                   |
| Data      | `d`  | object   | Event-specific payload, varies by `_t`         |

Run identity is not carried per-event. It is expressed by the containing directory (see §4).

Each event type is represented by a dedicated C# record type for compile-time safety. FE consumers may use dynamic typing if preferred.

### 2.2 Event Types

**Market data events:**
- `tick` — new tick received
- `bar` — new bar closed
- `bar.mut` — last (open) bar mutated (price update within forming bar)

**Indicator events:**
- `ind` — indicator value computed for a closed bar
- `ind.mut` — indicator value recomputed due to bar mutation

**Signal & risk events:**
- `sig` — signal generated by strategy logic
- `risk` — risk check performed (pass/reject + reason)

**Order lifecycle events:**
- `ord.place` — order submitted
- `ord.fill` — order filled (partial or full)
- `ord.cancel` — order cancelled
- `ord.reject` — order rejected
- `pos` — position changed

**System events:**
- `run.start` — run began (includes config/params snapshot)
- `run.end` — run completed (includes summary stats)
- `err` — error
- `warn` — warning

### 2.3 Export Mode Tags

Each event type is annotated in code with a non-exported `[Flags] enum ExportMode` that determines in which execution contexts the event is emitted:

```
[Flags]
enum ExportMode
{
    Backtest     = 1,
    Optimization = 2,
    Live         = 4,
}
```

Configuration is per event type, defined in code (not runtime config):

| Event type(s)                        | Default ExportMode               | Rationale                                         |
|--------------------------------------|----------------------------------|----------------------------------------------------|
| `ord.*`, `pos`                       | Backtest \| Optimization \| Live | Trade activity is always captured                  |
| `sig`, `risk`                        | Backtest \| Live                 | Decision logic — relevant for debugging, not for mass optimization |
| `bar`, `ind`                         | Backtest                         | Data-level events — debug only                     |
| `tick`                               | Backtest (opt-in)                | Very verbose, only when explicitly needed          |
| `bar.mut`, `ind.mut`                 | Backtest (opt-in)                | Mutation events generate high noise; rarely needed  |
| `run.start`, `run.end`              | Backtest \| Optimization \| Live | Always present as bookends                         |
| `err`, `warn`                        | Backtest \| Optimization \| Live | Always captured                                    |

The event bus checks `eventType.ExportMode.HasFlag(currentRunMode)` before serialization. Events that don't match are silently dropped — never serialized, never written to any sink.

### 2.4 Data Subscription Export Control

Bar and indicator export is governed by the `DataSubscription` configuration:

```csharp
public record DataSubscription(Asset Asset, TimeSpan TimeFrame, bool IsExportable = false);
```

A strategy declares one or more data subscriptions (e.g. ETH-USD M1, ETH-USD H1, BTC-USD H4). Only subscriptions with `IsExportable = true` have their `bar`, `bar.mut`, `ind`, and `ind.mut` events emitted to sinks. Non-exportable subscriptions are still processed internally by the strategy but produce no event output.

This allows fine-grained control in multi-asset, multi-timeframe strategies. For example, a strategy consuming M1 bars for signal calculation but operating on H1 decisions would mark only the H1 subscription as exportable — avoiding noise from 60x more M1 bar events.

The visual debugger draws and steps through bars from exportable subscriptions only. Post-run reports likewise use exportable subscriptions for charting.

---

## 3. Event Bus Architecture

### 3.1 Single Emission Point

Strategy code, indicators, execution engine, and risk manager all emit events through a single `IEventBus` abstraction. The bus:

- Checks `ExportMode` tag against the current run mode — drops non-matching events
- Applies `DataSubscription.IsExportable` filter for bar/indicator events — only emits from exportable subscriptions
- Serializes each surviving event to JSON **once**
- Fans out the serialized payload to all registered sinks

### 3.2 Sinks

| Sink | Purpose | Receives |
|------|---------|----------|
| **JSONL File Sink** | Post-hoc AI/human analysis | All exported events (after mode + TF filtering) |
| **WebSocket Sink** | Real-time FE visual debugger | All exported events (after mode + TF filtering) |

Trade logging is not a separate sink — trade events (`ord.*`, `pos`) flow through the same JSONL file sink as all other events. They are extracted into the Trade DB during post-run index build (see §6).

---

## 4. File Organization & Run Identity

The following structure should be in the same AlgoTradeForge dir (..AppData/localAlgoTradeForge on Windows) that contains partitioned Candle data from CandleIngestor.

### 4.1 Directory Structure

```
Data/
  EventLogs/
    {run_folder_name}/
      events.jsonl        # complete chronological event stream
      meta.json           # run config, params, summary (written at run.end)
      index.sqlite        # AI debug query index (built post-run, if enabled)
  trades.sqlite           # persistent trade DB, shared across all runs
```

### 4.2 Run Folder Naming

Run identity is encoded in the folder name, not in individual events. The folder name is a human-readable identifier:

```
{strategy_name}_v{version}_{asset}_{period}_{params_hash}_{timestamp}
```

Examples:
```
MeanRevert_v2.3_ETH-USD_H1_2024-2025_a3f8c1_20260208T143201
TrendFollow_v1.0_BTC-USD_M15_2025-2026_b7d2e4_20260208T150000
```

Components:
- `strategy_name` + `version` — human-readable identification
- `asset` + `period` — the primary backtest parameters
- `params_hash` — short hash of the full parameter set (for uniqueness when same strategy/asset runs with different params)
- `timestamp` — run start time (prevents collision on re-runs)

The full parameter set is stored inside `meta.json`, not in the folder name.

### 4.3 Parallel Run Isolation

Each concurrent run (e.g. during optimization) writes to its own directory. There is no shared file or stream between runs. Isolation is structural (separate directories), not logical (filtering by run ID within a shared stream).

---

## 5. WebSocket Interface (Visual Debugger)

### 5.1 Connection Model

The backtest runner hosts a WebSocket server. The FE client connects to a specific run's channel.

- One WebSocket connection per observed run
- Events are pushed to the client as they are emitted (same JSON as file sink)
- If no FE client is connected, the sink is a no-op (events still go to file)

### 5.2 Execution Control via WebSocket

The FE client sends **control messages** to govern execution flow. The runner starts in **paused** state when a visual debug session is active.

**Control commands:**

| Command | Payload | Behavior |
|---------|---------|----------|
| `continue` | — | Run to completion without pausing |
| `next` | — | Advance to the next exported event (any type), then pause |
| `next_type` | `{ "_t": "bar" }` | Advance until the next event of the given type is exported, then pause |
| `next_bar` | — | Shortcut: advance to next `bar` event from any exportable subscription, then pause |
| `next_signal` | — | Shortcut: advance to next `sig` event, then pause |
| `next_trade` | — | Advance until next order lifecycle event, then pause |
| `run_to` | `{ "sq": 5000 }` or `{ "ts": "..." }` | Run until a specific sequence number or timestamp, then pause |
| `set_export` | `{ "mutations": true }` | Toggle opt-in event categories mid-run (e.g. enable `bar.mut`, `ind.mut`) |
| `pause` | — | Pause execution after the current event completes |

All stepping commands operate on **exported** events only (post-filter). `next_bar` steps to the next bar from any exportable `DataSubscription`, skipping bars from non-exportable subscriptions.

### 5.3 Pause Semantics

When paused, the runner:

- Has fully processed and emitted the current event
- Holds all state (open positions, indicator buffers, current bar) in memory
- Waits for the next control command before advancing
- Remains responsive to WebSocket control messages

When `continue` is issued, the runner stops pausing between events and runs at full speed, still emitting events to all sinks in real-time.

### 5.4 No-Client Mode

When running without a visual debugger (normal backtest or optimization), the WebSocket server is either not started or execution is in `continue` mode by default (no pausing). Determined by run configuration.

---

## 6. Post-Run Persistence

Both persistent stores are built from the JSONL event stream after the run completes or is interrupted. During the run, only JSONL is written (plus WebSocket emission if active).

### 6.1 AI Debug Index (index.sqlite)

**Trigger:** built post-run **if enabled in backtest settings**. Not built for optimization runs by default.

**Source:** parses the run's `events.jsonl`.

**Schema:**
- `events` table — all events with extracted top-level fields (`sq`, `ts`, `_t`, `src`) as indexed columns + full JSON in a `raw` column
- Indexed on: `sq`, `ts`, `_t`, `src`

**Location:** `Data/EventLogs/{run_folder}/index.sqlite` — co-located with the JSONL it indexes.

**Crash resilience:** if the run terminates abnormally, the JSONL file is the only artifact. The index can be rebuilt later manually or by a utility command operating on the existing JSONL.

### 6.2 Trade DB (trades.sqlite)

**Trigger:** built post-run **always**, for every execution mode (backtest, optimization, live).

**Source:** extracts `ord.*` and `pos` events from the run's `events.jsonl`.

**Schema:**
- `runs` table — run folder name, strategy, version, asset, period, full params (JSON), start/end time, mode, summary stats
- `orders` table — FK to run, full order lifecycle
- `trades` table — FK to order, individual fills

**Location:** `Data/trades.sqlite` — single shared file across all runs.

**Write behavior:** inserts transactionally after the run completes. For optimization with many parallel runs, each run's trade data is inserted in a single transaction upon that run's completion.

**Crash resilience:** if the run terminates abnormally, trade data can still be recovered by re-extracting from the JSONL (same as index build).

### 6.3 Lifecycle Summary

```
During run:      events.jsonl ← append (+ WebSocket push if visual debug)
                 meta.json ← written at run.end

After run:       events.jsonl → index.sqlite    (if enabled)
                 events.jsonl → trades.sqlite   (always)
```

---

## 7. Execution Modes Summary

| Mode | JSONL Export | AI Debug Index | Trade DB | WebSocket | Default Export |
|------|-------------|----------------|----------|-----------|----------------|
| **Debug (visual)** | ✓ | ✓ | ✓ | ✓ (paused start) | `Backtest` flag events |
| **Debug (AI/headless)** | ✓ | ✓ | ✓ | ✗ | `Backtest` flag events |
| **Backtest** | ✓ | Optional | ✓ | ✗ | `Backtest` flag events |
| **Optimization** | ✓ (trades only) | ✗ | ✓ | ✗ | `Optimization` flag events |
| **Prod / Paper** | ✓ | ✗ | ✓ | ✗ | `Live` flag events |

---

## 8. AI Debugger Experience (Claude Code)

### 8.1 Available Artifacts Per Run

Claude Code has access to:

1. `meta.json` — read first, understand what the run was
2. `events.jsonl` — sequential reading for narrative debugging
3. `index.sqlite` — SQL queries for analytical debugging (if built)
4. Skill file documenting the event schema, file layout, and recommended workflows

### 8.2 Expected Debugging Workflows

- **Triage:** read `meta.json` summary → query `index.sqlite` for error/warning count → read those events from JSONL for context
- **Execution debugging:** query orders from `index.sqlite` → find sequence range → read surrounding events from JSONL to see what led to the order
- **Indicator debugging:** query specific indicator values from `index.sqlite` → compare expected vs actual → read raw bar data around anomalies
- **Comparison across runs:** query `index.sqlite` from two runs → diff trade outcomes → investigate divergence points in JSONL

### 8.3 Skill File Contract

A skill file (`SKILL.md`) is provided that documents:
- Event envelope schema and compact field names
- All event types and their `d` (data) payload shapes
- File layout and naming conventions
- Recommended `jq` and `sqlite3` query patterns
- Debugging workflow decision tree (start here → then check this → then look at that)

---

## 9. Design Decisions Log

| # | Decision | Rationale |
|---|----------|-----------|
| 1 | Flag enum `ExportMode` over logging levels | Export is per-event-type configuration, not a severity hierarchy. A bar event isn't "less important" than a trade — it's relevant in different contexts. |
| 2 | `DataSubscription.IsExportable` flag over global reporting timeframe | Per-subscription export control supports multi-asset, multi-TF strategies naturally. No single "reporting TF" assumption — each subscription decides independently. |
| 3 | No run ID in event payload | Run identity is structural (directory), not per-event metadata. Eliminates redundant bytes and simplifies the event model. |
| 4 | Single chronological JSONL, no partitioning | Preserves narrative flow for LLM sequential reasoning. SQLite index provides type-based access post-hoc. |
| 5 | JSONL as source of truth, SQLite as derived | Simplifies the write path (append-only file). Both indexes are built from the same source after run completion. |
| 6 | Trade DB shared across runs | Enables cross-run queries (all ETH-USD trades across strategy versions) without opening N directories. |
| 7 | Strictly typed C# records per event type | Compile-time safety for event payloads. FE may use dynamic typing independently. |
| 8 | Hybrid emission: explicit `IEventBus` injection + source-generated indicator proxies | Keeps business-critical emissions visible in code while eliminating boilerplate from pure-computation indicators. See §10. |

---

## 10. Event Emission API — Hybrid Approach

### 10.1 Design Goal

Event emission is a cross-cutting concern that touches strategy code, the execution engine, risk management, and indicators. The API must be:

- **Lightweight** — minimal ceremony at the call site
- **Explicit where it matters** — business-critical events (signals, orders, risk decisions) should be visible in the code that produces them
- **Invisible where it doesn't** — indicators are pure computations; they should not know events exist

### 10.2 Two Emission Mechanisms

| Emission point | Mechanism | Rationale |
|---|---|---|
| Strategy signals (`sig`) | Explicit `IEventBus.Emit()` call | Signals are business decisions — the developer should see the emission in code |
| Risk checks (`risk`) | Explicit `IEventBus.Emit()` call | Same — these are judgment calls, not boilerplate |
| Order lifecycle (`ord.*`, `pos`) | Explicit `IEventBus.Emit()` call | The execution engine explicitly decides when orders change state |
| Bar events (`bar`, `bar.mut`) | Explicit `IEventBus.Emit()` call | The backtest engine owns the bar loop — natural single emission point |
| System events (`run.*`, `err`, `warn`) | Explicit `IEventBus.Emit()` call | Infrastructure-level, emitted by the engine at well-defined lifecycle points |
| Indicator values (`ind`, `ind.mut`) | **Automatic via source-generated proxy** | Indicators follow a uniform `Compute(bar) → value` contract; emission is mechanical and identical for every indicator |

### 10.3 Explicit Injection Path

Components that emit business-critical events receive `IEventBus` via standard constructor injection:

```csharp
public interface IEventBus
{
    void Emit<T>(T evt) where T : IBacktestEvent;
}
```

The bus implementation handles all filtering internally (`ExportMode` check, `DataSubscription.IsExportable` check). Call sites never check filters — they emit unconditionally and the bus drops what doesn't match. This keeps call sites to a single line.

The classes that receive `IEventBus` are few and well-defined: the backtest engine, the execution engine, the risk manager, and the strategy base class. This is not a "sprinkled everywhere" dependency.

### 10.4 Source-Generated Indicator Proxy Path

Indicators implement a pure computation interface with no event awareness:

```csharp
[EmitOnCompute("ind")]
public interface IIndicator<TOut>
{
    string Name { get; }
    TOut Compute(Int64Bar bar);
    void Reset();
}
```

The `[EmitOnCompute]` attribute marks the interface for source generation. At compile time, a source generator produces a thin wrapper (decorator) for each indicator interface that:

1. Delegates all calls to the inner (real) indicator
2. After `Compute()` returns, emits an `ind` event carrying the indicator name, the result value, and the associated `DataSubscription`
3. Passes through all other members (`Name`, `Reset`) without interception

The generated wrapper is injected by the engine at run setup time: the engine takes the strategy's raw indicators, wraps each one with its generated emitting counterpart, and binds the wrapper to the correct `DataSubscription` and `IEventBus` instance. The strategy never sees the wrapper — it holds the same `IIndicator<T>` reference.

**Key property:** indicators remain pure Domain types with zero dependencies on the event system. They can be unit-tested in complete isolation. The emission concern is handled entirely by the generated infrastructure code.

### 10.5 Why Source Generation over DispatchProxy

.NET provides a built-in runtime proxy mechanism (`System.Reflection.DispatchProxy`) that could achieve the same wrapping without code generation. However, it has three problems in a hot backtest loop processing millions of bars:

1. **Reflection overhead** — every proxied call goes through `MethodInfo`-based dispatch. The proxy must inspect which method was called on every invocation.
2. **Argument boxing** — `Int64Bar` is a `record struct`. The proxy receives arguments as `object?[]`, forcing a heap allocation per call.
3. **Return value boxing** — numeric indicator results (e.g. `double`) are boxed on return through the proxy.

For a backtest processing 500K+ bars with 3–5 indicators each, this creates millions of unnecessary allocations and reflection lookups per run.

A **source generator** eliminates all three problems. The generated wrapper is a concrete class with strongly-typed method signatures — no reflection, no boxing, no allocation beyond the event record itself. The wrapper is as efficient as hand-written code because it *is* hand-written code, just authored by the compiler.

### 10.6 Source Generator Shape

The source generator:

- **Trigger:** scans for interfaces annotated with `[EmitOnCompute("eventType")]`
- **Input:** the interface definition (method signatures, generic type parameters)
- **Output:** a concrete wrapper class per interface that implements the same interface

For the `IIndicator<TOut>` interface, the generator produces a class equivalent to:

```
class Emitting_IIndicator<TOut> : IIndicator<TOut>
    ctor(inner: IIndicator<TOut>, bus: IEventBus, subscription: DataSubscription)
    Name       → delegates to inner.Name
    Compute()  → calls inner.Compute(), then bus.Emit(new IndicatorEvent(...)), returns result
    Reset()    → delegates to inner.Reset()
```

The generator only intercepts methods explicitly marked or matching the `Compute` convention. All other interface members are pure pass-through.

A factory method is also generated for convenient wrapping:

```
static IIndicator<TOut> WithEmitting(this IIndicator<TOut> inner, IEventBus bus, DataSubscription sub)
```

This allows the engine setup code to wrap indicators in a single expression.

### 10.7 Opt-Out for Optimization Runs

In optimization mode, `ind` events are not in the `ExportMode` filter — the bus drops them. But the overhead of *creating* the event record before the bus drops it is still wasteful at scale.

Two levels of opt-out:

1. **Bus-level drop** (default) — the generated wrapper always calls `bus.Emit()`, and the bus's `ExportMode` check rejects it before serialization. Minimal overhead (one record allocation + one flag check per indicator per bar).
2. **Skip wrapping entirely** — during optimization run setup, the engine does not wrap indicators with emitting proxies at all. Zero overhead. The strategy holds raw indicator references. This is a setup-time decision, not a per-call branch.