# Backtest Debug & Event Export System — Requirements

## 1. Overview

A unified event export system that serves three consumers from a single event stream:

1. **Visual Debugger** — real-time FE client connected via WebSocket, interactive step-through
2. **AI Debugger (Claude Code)** — post-hoc analysis via JSONL files + SQLite index
3. **Trade Logger** — persistent trade/order records, streamed as JSONL during the run, persisted to SQLite after run completes

The system supports multiple execution modes with different export filters, and handles concurrent parallel runs by writing to separate output directories.

---

## 2. Event Model

### 2.1 Canonical Event Shape

All events share a minimal envelope with compact field names:

| Field     | Key  | Type     | Description                                    |
|-----------|------|----------|------------------------------------------------|
| Timestamp | `ts` | ISO 8601 | UTC time of emission                           |
| Sequence  | `sq` | uint64   | Monotonic per-run, guarantees ordering         |
| Type      | `_t` | string   | Event type identifier (see §2.2)               |
| Source    | `src`| string   | Emitting component/subsystem                   |
| Data      | `d`  | object   | Event-specific payload, varies by `_t`         |

Run identity is not carried per-event. It is expressed by the containing directory (see §4).

Each event type is represented by a dedicated C# record type for compile-time safety. FE consumers may use dynamic typing if preferred.

### 2.2 Event Types

**Market data events:**
- `tick` — new tick received
- `bar` — new bar closed
- `bar.mut` — last (open) bar mutated (price update within forming bar)

**Indicator events:**
- `ind` — indicator value computed for a closed bar
- `ind.mut` — indicator value recomputed due to bar mutation

**Signal & risk events:**
- `sig` — signal generated by strategy logic
- `risk` — risk check performed (pass/reject + reason)

**Order lifecycle events:**
- `ord.place` — order submitted
- `ord.fill` — order filled (partial or full)
- `ord.cancel` — order cancelled
- `ord.reject` — order rejected
- `pos` — position changed

**System events:**
- `run.start` — run began (includes config/params snapshot)
- `run.end` — run completed (includes summary stats)
- `err` — error
- `warn` — warning

### 2.3 Export Mode Tags

Each event type is annotated in code with a non-exported `[Flags] enum ExportMode` that determines in which execution contexts the event is emitted:

```
[Flags]
enum ExportMode
{
    Backtest     = 1,
    Optimization = 2,
    Live         = 4,
}
```

Configuration is per event type, defined in code (not runtime config):

| Event type(s)                        | Default ExportMode               | Rationale                                         |
|--------------------------------------|----------------------------------|----------------------------------------------------|
| `ord.*`, `pos`                       | Backtest \| Optimization \| Live | Trade activity is always captured                  |
| `sig`, `risk`                        | Backtest \| Live                 | Decision logic — relevant for debugging, not for mass optimization |
| `bar`, `ind`                         | Backtest                         | Data-level events — debug only                     |
| `tick`                               | Backtest (opt-in)                | Very verbose, only when explicitly needed          |
| `bar.mut`, `ind.mut`                 | Backtest (opt-in)                | Mutation events generate high noise; rarely needed  |
| `run.start`, `run.end`              | Backtest \| Optimization \| Live | Always present as bookends                         |
| `err`, `warn`                        | Backtest \| Optimization \| Live | Always captured                                    |

The event bus checks `eventType.ExportMode.HasFlag(currentRunMode)` before serialization. Events that don't match are silently dropped — never serialized, never written to any sink.

### 2.4 Data Subscription Export Control

Bar and indicator export is governed by the `DataSubscription` configuration:

```csharp
public record DataSubscription(Asset Asset, TimeSpan TimeFrame, bool IsExportable = false);
```

A strategy declares one or more data subscriptions (e.g. ETH-USD M1, ETH-USD H1, BTC-USD H4). Only subscriptions with `IsExportable = true` have their `bar`, `bar.mut`, `ind`, and `ind.mut` events emitted to sinks. Non-exportable subscriptions are still processed internally by the strategy but produce no event output.

This allows fine-grained control in multi-asset, multi-timeframe strategies. For example, a strategy consuming M1 bars for signal calculation but operating on H1 decisions would mark only the H1 subscription as exportable — avoiding noise from 60x more M1 bar events.

The visual debugger draws and steps through bars from exportable subscriptions only. Post-run reports likewise use exportable subscriptions for charting.

---

## 3. Event Bus Architecture

### 3.1 Single Emission Point

Strategy code, indicators, execution engine, and risk manager all emit events through a single `IEventBus` abstraction. The bus:

- Checks `ExportMode` tag against the current run mode — drops non-matching events
- Applies `DataSubscription.IsExportable` filter for bar/indicator events — only emits from exportable subscriptions
- Serializes each surviving event to JSON **once**
- Fans out the serialized payload to all registered sinks

### 3.2 Sinks

| Sink | Purpose | Receives |
|------|---------|----------|
| **JSONL File Sink** | Post-hoc AI/human analysis | All exported events (after mode + TF filtering) |
| **WebSocket Sink** | Real-time FE visual debugger | All exported events (after mode + TF filtering) |

Trade logging is not a separate sink — trade events (`ord.*`, `pos`) flow through the same JSONL file sink as all other events. They are extracted into the Trade DB during post-run index build (see §6).

---

## 4. File Organization & Run Identity

The following structure should be in the same AlgoTradeForge dir (..AppData/localAlgoTradeForge on Windows) that contains partitioned Candle data from CandleIngestor.

### 4.1 Directory Structure

```
Data/
  EventLogs/
    {run_folder_name}/
      events.jsonl        # complete chronological event stream
      meta.json           # run config, params, summary (written at run.end)
      index.sqlite        # AI debug query index (built post-run, if enabled)
  trades.sqlite           # persistent trade DB, shared across all runs
```

### 4.2 Run Folder Naming

Run identity is encoded in the folder name, not in individual events. The folder name is a human-readable identifier:

```
{strategy_name}_v{version}_{asset}_{period}_{params_hash}_{timestamp}
```

Examples:
```
MeanRevert_v2.3_ETH-USD_H1_2024-2025_a3f8c1_20260208T143201
TrendFollow_v1.0_BTC-USD_M15_2025-2026_b7d2e4_20260208T150000
```

Components:
- `strategy_name` + `version` — human-readable identification
- `asset` + `period` — the primary backtest parameters
- `params_hash` — short hash of the full parameter set (for uniqueness when same strategy/asset runs with different params)
- `timestamp` — run start time (prevents collision on re-runs)

The full parameter set is stored inside `meta.json`, not in the folder name.

### 4.3 Parallel Run Isolation

Each concurrent run (e.g. during optimization) writes to its own directory. There is no shared file or stream between runs. Isolation is structural (separate directories), not logical (filtering by run ID within a shared stream).

---

## 5. WebSocket Interface (Visual Debugger)

### 5.1 Connection Model

The backtest runner hosts a WebSocket server. The FE client connects to a specific run's channel.

- One WebSocket connection per observed run
- Events are pushed to the client as they are emitted (same JSON as file sink)
- If no FE client is connected, the sink is a no-op (events still go to file)

### 5.2 Execution Control via WebSocket

The FE client sends **control messages** to govern execution flow. The runner starts in **paused** state when a visual debug session is active.

**Control commands:**

| Command | Payload | Behavior |
|---------|---------|----------|
| `continue` | — | Run to completion without pausing |
| `next` | — | Advance to the next exported event (any type), then pause |
| `next_type` | `{ "_t": "bar" }` | Advance until the next event of the given type is exported, then pause |
| `next_bar` | — | Shortcut: advance to next `bar` event from any exportable subscription, then pause |
| `next_signal` | — | Shortcut: advance to next `sig` event, then pause |
| `next_trade` | — | Advance until next order lifecycle event, then pause |
| `run_to` | `{ "sq": 5000 }` or `{ "ts": "..." }` | Run until a specific sequence number or timestamp, then pause |
| `set_export` | `{ "mutations": true }` | Toggle opt-in event categories mid-run (e.g. enable `bar.mut`, `ind.mut`) |
| `pause` | — | Pause execution after the current event completes |

All stepping commands operate on **exported** events only (post-filter). `next_bar` steps to the next bar from any exportable `DataSubscription`, skipping bars from non-exportable subscriptions.

### 5.3 Pause Semantics

When paused, the runner:

- Has fully processed and emitted the current event
- Holds all state (open positions, indicator buffers, current bar) in memory
- Waits for the next control command before advancing
- Remains responsive to WebSocket control messages

When `continue` is issued, the runner stops pausing between events and runs at full speed, still emitting events to all sinks in real-time.

### 5.4 No-Client Mode

When running without a visual debugger (normal backtest or optimization), the WebSocket server is either not started or execution is in `continue` mode by default (no pausing). Determined by run configuration.

---

## 6. Post-Run Persistence

Both persistent stores are built from the JSONL event stream after the run completes or is interrupted. During the run, only JSONL is written (plus WebSocket emission if active).

### 6.1 AI Debug Index (index.sqlite)

**Trigger:** built post-run **if enabled in backtest settings**. Not built for optimization runs by default.

**Source:** parses the run's `events.jsonl`.

**Schema:**
- `events` table — all events with extracted top-level fields (`sq`, `ts`, `_t`, `src`) as indexed columns + full JSON in a `raw` column
- Indexed on: `sq`, `ts`, `_t`, `src`

**Location:** `Data/EventLogs/{run_folder}/index.sqlite` — co-located with the JSONL it indexes.

**Crash resilience:** if the run terminates abnormally, the JSONL file is the only artifact. The index can be rebuilt later manually or by a utility command operating on the existing JSONL.

### 6.2 Trade DB (trades.sqlite)

**Trigger:** built post-run **always**, for every execution mode (backtest, optimization, live).

**Source:** extracts `ord.*` and `pos` events from the run's `events.jsonl`.

**Schema:**
- `runs` table — run folder name, strategy, version, asset, period, full params (JSON), start/end time, mode, summary stats
- `orders` table — FK to run, full order lifecycle
- `trades` table — FK to order, individual fills

**Location:** `Data/trades.sqlite` — single shared file across all runs.

**Write behavior:** inserts transactionally after the run completes. For optimization with many parallel runs, each run's trade data is inserted in a single transaction upon that run's completion.

**Crash resilience:** if the run terminates abnormally, trade data can still be recovered by re-extracting from the JSONL (same as index build).

### 6.3 Lifecycle Summary

```
During run:      events.jsonl ← append (+ WebSocket push if visual debug)
                 meta.json ← written at run.end

After run:       events.jsonl → index.sqlite    (if enabled)
                 events.jsonl → trades.sqlite   (always)
```

---

## 7. Execution Modes Summary

| Mode | JSONL Export | AI Debug Index | Trade DB | WebSocket | Default Export |
|------|-------------|----------------|----------|-----------|----------------|
| **Debug (visual)** | ✓ | ✓ | ✓ | ✓ (paused start) | `Backtest` flag events |
| **Debug (AI/headless)** | ✓ | ✓ | ✓ | ✗ | `Backtest` flag events |
| **Backtest** | ✓ | Optional | ✓ | ✗ | `Backtest` flag events |
| **Optimization** | ✓ (trades only) | ✗ | ✓ | ✗ | `Optimization` flag events |
| **Prod / Paper** | ✓ | ✗ | ✓ | ✗ | `Live` flag events |

---

## 8. AI Debugger Experience (Claude Code)

### 8.1 Available Artifacts Per Run

Claude Code has access to:

1. `meta.json` — read first, understand what the run was
2. `events.jsonl` — sequential reading for narrative debugging
3. `index.sqlite` — SQL queries for analytical debugging (if built)
4. Skill file documenting the event schema, file layout, and recommended workflows

### 8.2 Expected Debugging Workflows

- **Triage:** read `meta.json` summary → query `index.sqlite` for error/warning count → read those events from JSONL for context
- **Execution debugging:** query orders from `index.sqlite` → find sequence range → read surrounding events from JSONL to see what led to the order
- **Indicator debugging:** query specific indicator values from `index.sqlite` → compare expected vs actual → read raw bar data around anomalies
- **Comparison across runs:** query `index.sqlite` from two runs → diff trade outcomes → investigate divergence points in JSONL

### 8.3 Skill File Contract

A skill file (`SKILL.md`) is provided that documents:
- Event envelope schema and compact field names
- All event types and their `d` (data) payload shapes
- File layout and naming conventions
- Recommended `jq` and `sqlite3` query patterns
- Debugging workflow decision tree (start here → then check this → then look at that)

---

## 9. Design Decisions Log

| # | Decision | Rationale |
|---|----------|-----------|
| 1 | Flag enum `ExportMode` over logging levels | Export is per-event-type configuration, not a severity hierarchy. A bar event isn't "less important" than a trade — it's relevant in different contexts. |
| 2 | `DataSubscription.IsExportable` flag over global reporting timeframe | Per-subscription export control supports multi-asset, multi-TF strategies naturally. No single "reporting TF" assumption — each subscription decides independently. |
| 3 | No run ID in event payload | Run identity is structural (directory), not per-event metadata. Eliminates redundant bytes and simplifies the event model. |
| 4 | Single chronological JSONL, no partitioning | Preserves narrative flow for LLM sequential reasoning. SQLite index provides type-based access post-hoc. |
| 5 | JSONL as source of truth, SQLite as derived | Simplifies the write path (append-only file). Both indexes are built from the same source after run completion. |
| 6 | Trade DB shared across runs | Enables cross-run queries (all ETH-USD trades across strategy versions) without opening N directories. |
| 7 | Strictly typed C# records per event type | Compile-time safety for event payloads. FE may use dynamic typing independently. |
| 8 | Hybrid emission: explicit `IEventBus` injection + factory-decorated indicators | Keeps business-critical emissions visible in code while eliminating boilerplate from pure-computation indicators. Factory earns dual justification: event emission control and centralized indicator construction for optimization sweeps. See §10. |
| 9 | `IIndicatorFactory` + generic decorator over source-generated proxies | Strategies create indicators directly and hold concrete/interface references. A factory provides the interception point for wrapping without engine needing visibility into strategy internals. One hand-written generic decorator covers all indicator types — no source generation complexity. See §10.4–10.6. |

---

## 10. Event Emission API — Hybrid Approach

### 10.1 Design Goal

Event emission is a cross-cutting concern that touches strategy code, the execution engine, risk management, and indicators. The API must be:

- **Lightweight** — minimal ceremony at the call site
- **Explicit where it matters** — business-critical events (signals, orders, risk decisions) should be visible in the code that produces them
- **Invisible where it doesn't** — indicators are pure computations; they should not know events exist

### 10.2 Two Emission Mechanisms

| Emission point | Mechanism | Rationale |
|---|---|---|
| Strategy signals (`sig`) | Explicit `IEventBus.Emit()` call | Signals are business decisions — the developer should see the emission in code |
| Risk checks (`risk`) | Explicit `IEventBus.Emit()` call | Same — these are judgment calls, not boilerplate |
| Order lifecycle (`ord.*`, `pos`) | Explicit `IEventBus.Emit()` call | The execution engine explicitly decides when orders change state |
| Bar events (`bar`, `bar.mut`) | Explicit `IEventBus.Emit()` call | The backtest engine owns the bar loop — natural single emission point |
| System events (`run.*`, `err`, `warn`) | Explicit `IEventBus.Emit()` call | Infrastructure-level, emitted by the engine at well-defined lifecycle points |
| Indicator values (`ind`, `ind.mut`) | **Automatic via factory-decorated indicator** | Indicators follow a uniform `Compute(series) → values` contract; emission is mechanical and identical for every indicator. A factory wraps indicators with a generic decorator at creation time. |

### 10.3 Explicit Injection Path

Components that emit business-critical events receive `IEventBus` via standard constructor injection:

```csharp
public interface IEventBus
{
    void Emit<T>(T evt) where T : IBacktestEvent;
}
```

The bus implementation handles all filtering internally (`ExportMode` check, `DataSubscription.IsExportable` check). Call sites never check filters — they emit unconditionally and the bus drops what doesn't match. This keeps call sites to a single line.

The classes that receive `IEventBus` are few and well-defined: the backtest engine, the execution engine, the risk manager, and the strategy base class. This is not a "sprinkled everywhere" dependency.

### 10.4 Indicator Factory Path

Strategies create indicators through an injected `IIndicatorFactory`. The factory is the interception point for event emission — it wraps indicators with a generic decorator that emits `ind` events after each `Compute()` call.

```csharp
public interface IIndicatorFactory
{
    IIndicator<TInp, TBuff> Create<TInp, TBuff>(
        IIndicator<TInp, TBuff> indicator,
        DataSubscription subscription);
}
```

Strategy code creates indicators by passing a raw instance and the associated subscription:

```csharp
public sealed class ZigZagBreakoutStrategy(
    ZigZagBreakoutParams parameters,
    IIndicatorFactory indicators) : StrategyBase<ZigZagBreakoutParams>(parameters)
{
    private IIndicator<Int64Bar, long> _dzz = null!;

    public override void OnInit()
    {
        _dzz = indicators.Create(
            new DeltaZigZag(Params.DzzDepth / 10m, Params.MinimumThreshold),
            DataSubscriptions[0]);
    }

    public override void OnBar(Int64Bar bar, DataSubscription subscription, IOrderContext orders)
    {
        _barHistory.Add(bar);
        _dzz.Compute(_barHistory);          // decorator emits ind event here
        var values = _dzz.Buffers["Value"];  // works through interface
        // ... strategy logic unchanged
    }
}
```

The strategy holds an `IIndicator<TInp, TBuff>` interface reference. All existing indicator interface members (`Buffers`, `Name`, `Measure`, `Compute`) are accessible through the interface — no concrete type access is lost for strategy logic.

**Key property:** indicators remain pure Domain types with zero dependencies on the event system. They can be unit-tested in complete isolation. The emission concern is handled entirely by the factory and decorator infrastructure.

### 10.5 Generic Decorator

A single hand-written generic decorator class covers all indicator types — no source generation required:

```csharp
internal sealed class EmittingIndicator<TInp, TBuff>(
    IIndicator<TInp, TBuff> inner,
    IEventBus bus,
    DataSubscription sub) : IIndicator<TInp, TBuff>
{
    public string Name => inner.Name;
    public IndicatorMeasure Measure => inner.Measure;
    public IReadOnlyDictionary<string, IReadOnlyList<TBuff>> Buffers => inner.Buffers;
    public int MinimumHistory => inner.MinimumHistory;
    public int? CapacityLimit => inner.CapacityLimit;

    public void Compute(IReadOnlyList<TInp> series)
    {
        inner.Compute(series);
        bus.Emit(new IndEvent(inner.Name, inner.Measure, /* latest buffer values */, sub));
    }
}
```

The decorator is strongly typed — no reflection, no boxing of `Int64Bar` (a `record struct`), no allocation beyond the event record itself. Performance is identical to hand-written delegation code.

**Why not source generation:** The original design considered a Roslyn source generator to auto-produce decorator classes per indicator interface. This was rejected because:

1. **One generic class suffices** — `IIndicator<TInp, TBuff>` is the only indicator interface. A single `EmittingIndicator<TInp, TBuff>` covers all concrete indicators without code generation.
2. **No interception point existed** — source-gen proxies assumed the engine would wrap indicators at setup time, but strategies create indicators internally. The factory pattern provides this interception point explicitly, making the source generator's wrapping mechanism redundant.
3. **Build complexity** — source generators add a project dependency, complicate debugging, and require maintenance of generator code. A plain class in the infrastructure layer is simpler.

**Why not `DispatchProxy`:** .NET's runtime proxy mechanism (`System.Reflection.DispatchProxy`) suffers from reflection overhead, argument boxing (`Int64Bar` is a `record struct` → heap allocation per call), and return value boxing. For 500K+ bars × 3–5 indicators, this creates millions of unnecessary allocations. The hand-written generic decorator avoids all three.

### 10.6 Factory Implementations

Two factory implementations, selected at run setup time:

```csharp
// Backtest / debug — wraps with emission decorator
internal sealed class EmittingIndicatorFactory(IEventBus bus) : IIndicatorFactory
{
    public IIndicator<TInp, TBuff> Create<TInp, TBuff>(
        IIndicator<TInp, TBuff> indicator, DataSubscription sub)
        => new EmittingIndicator<TInp, TBuff>(indicator, bus, sub);
}

// Optimization — pass-through, zero overhead
internal sealed class PassthroughIndicatorFactory : IIndicatorFactory
{
    public IIndicator<TInp, TBuff> Create<TInp, TBuff>(
        IIndicator<TInp, TBuff> indicator, DataSubscription sub)
        => indicator;
}
```

The engine selects the factory at run setup based on execution mode. This is a one-time decision — no per-call branches.

### 10.7 Multi-Subscription Correctness

In multi-asset or multi-timeframe strategies (e.g. ETH M1 + ETH H1 with different indicators per timeframe), indicator event emission must fire only when an indicator actually computes — not on every `OnBar` call.

The decorator achieves this by construction: the `ind` event is emitted inside `Compute()`, so it fires if and only if the strategy calls `Compute` on that indicator for the current bar. If the strategy only computes the H1 indicator on H1 bars, only H1 bars produce `ind` events for that indicator.

This is a correctness advantage over alternative approaches (e.g. emitting all tracked indicators from `StrategyBase.OnBar`) which would require manual association of indicators to subscriptions to avoid emitting stale snapshots.

### 10.8 Opt-Out for Optimization Runs

In optimization mode, `ind` events are not in the `ExportMode` filter — the bus would drop them. But the overhead of *creating* the event record before the bus drops it is still wasteful at scale.

The factory approach provides clean opt-out: the engine injects `PassthroughIndicatorFactory` for optimization runs. The factory returns the raw indicator unwrapped — zero overhead, no allocation, no flag check per bar. This is a setup-time decision, not a per-call branch.

### 10.9 Dual Justification for the Factory

The `IIndicatorFactory` abstraction earns its existence for two independent reasons:

1. **Event emission control** — wrapping/not-wrapping indicators with the emitting decorator based on execution mode.
2. **Optimization module composition** — the optimization framework (see `optimizeable_params_framework.md` §7) supports indicators as pluggable `[OptimizableModule]` slots on strategy params. When the optimization runner resolves a module variant (e.g., SMA with Period=20 vs EMA with Period=20, Smoothing=0.2), it creates a raw `IIndicator<TInp, TBuff>` instance and injects it into the strategy params. The strategy then passes it through `IIndicatorFactory.Create()` — the same code path as manually constructed indicators. This means `IIndicatorFactory` is the **universal decoration point** regardless of how the indicator was created:

```
Manual construction:   new DeltaZigZag(...)  ──→ IIndicatorFactory.Create() ──→ wrapped/passthrough
Module injection:      registry.Create(...)  ──→ IIndicatorFactory.Create() ──→ wrapped/passthrough
```

The two features compose without coupling: the optimization framework owns indicator **selection and construction**, while `IIndicatorFactory` owns **decoration for event emission**. Neither knows about the other.

---

## 11. Implementation Roadmap

### 11.0 Dependency Graph

```
Phase 0 (done)
    │
Phase 1 (Event Model + Bus)
    │
Phase 2 (JSONL Sink + Run Identity)
    │
Phase 3 (Explicit Event Emission)
    ├───────────────┬───────────────┐
Phase 4             Phase 5         Phase 6
(Indicator Factory) (Post-Run SQL)  (WebSocket)
    │               │               │
    └───────────────┴───────┬───────┘
                            │
                      Phase 7
                  (Per-Event Stepping)
```

Phases 4, 5, and 6 are parallelizable after Phase 3 completes. Phase 7 depends on the event bus (Phase 1) plus at least one emission phase to be useful.

**Explicitly deferred:** Prod / Paper execution mode (§7 row 5) is not covered by any phase. The `ExportMode.Live` flag and filtering logic are implemented in Phase 1/3, but wiring a live execution path with event export is out of scope for the debug feature and will be addressed when the live trading engine is built.

### 11.1 Phase 0 — Debug Probe & Control Loop ✅ DONE

**Branch:** `007-debug-control-in-loop` | **Spec:** `docs/debug-feature/phase0-control-loop-plan.md`

Execution control via an injected `IDebugProbe` that gates the `BacktestEngine.Run()` loop at **bar boundaries**. Event serialization (JSONL, IEventBus) and WebSocket transport are out of scope for this phase.

**Commands implemented:**

| Command | Break Condition | Description |
|---------|----------------|-------------|
| `next_bar` | `Always` | Step to the next bar (any subscription) |
| `next` | `OnExportableBar` | Step to the next bar from an exportable subscription |
| `next_trade` | `OnFillBar` | Step to the next bar that produces fills |
| `run_to_sequence` | `AtSequence(n)` | Run until sequence number >= n |
| `run_to_timestamp` | `AtTimestamp(ms)` | Run until bar timestamp >= ms |
| `continue` | `Never` | Run to completion without pausing |
| `pause` | `Always` | Pause at the next bar boundary |

**HTTP endpoints (temporary POC — replaced by WebSocket in Phase 6):**

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/api/debug-sessions/` | Start a debug session (engine starts paused) |
| `POST` | `/api/debug-sessions/{id}/commands` | Send a control command, returns `DebugSnapshot` |
| `GET` | `/api/debug-sessions/{id}` | Get session status + last snapshot |
| `DELETE` | `/api/debug-sessions/{id}` | Terminate and clean up session |

**Files added/modified:**

| File | Layer | Change |
|------|-------|--------|
| `IDebugProbe.cs` | Domain | Interface: `IsActive`, `OnRunStart`, `OnBarProcessed`, `OnRunEnd` |
| `DebugSnapshot.cs` | Domain | `readonly record struct` with sequence, timestamp, subscription index, exportable flag, fill count, equity |
| `NullDebugProbe.cs` | Domain | Singleton null-object (`IsActive = false`) for non-debug runs |
| `BacktestEngine.cs` | Domain | Added optional `IDebugProbe?` param + 3 guarded call sites |
| `BacktestOrderContext.cs` | Domain | Extracted from engine for fill tracking |
| `DebugCommand.cs` | Application | Sealed record hierarchy for all commands |
| `BreakCondition.cs` | Application | Abstract record with `ShouldBreak(DebugSnapshot)` |
| `GatingDebugProbe.cs` | Application | `ManualResetEventSlim`-based gate, `SendCommandAsync` for HTTP thread |
| `DebugSession.cs` | Application | Session object owning probe + background `Task` |
| `IDebugSessionStore.cs` | Application | Session store interface |
| `InMemoryDebugSessionStore.cs` | Application | `ConcurrentDictionary`-based store |
| `DebugSessionDto.cs` | Application | DTOs for HTTP responses |
| `StartDebugSessionCommand[Handler].cs` | Application | Creates session, launches engine on `LongRunning` thread |
| `SendDebugCommand[Handler].cs` | Application | Sends command to probe, returns snapshot |
| `DebugContracts.cs` | WebApi | HTTP request records |
| `DebugEndpoints.cs` | WebApi | REST endpoints (temporary POC) |

### 11.2 Phase 1 — Event Model & Bus Core

**Status: NOT STARTED** | **Depends on:** Phase 0 | **Checklist:** `docs/debug-feature/phase1-checklist.md`

Define the event type system and the routing/filtering bus. No emission points or sinks — this phase builds the plumbing.

**Scope (references §2, §3, §10.3):**
- Canonical event envelope with compact field names (`ts`, `sq`, `_t`, `src`, `d`) — §2.1
- All event type C# records implementing a shared `IBacktestEvent` marker — §2.2
- `[Flags] enum ExportMode { Backtest = 1, Optimization = 2, Live = 4 }` with per-type annotations — §2.3
- `IEventBus` interface in Domain with single `Emit<T>(T evt)` method — §10.3
- `EventBus` implementation in Application: checks `ExportMode` vs current run mode, checks `DataSubscription.IsExportable` for bar/ind events, serializes once, fans out to registered `ISink` instances — §3.1
- `ISink` interface for fan-out targets
- `NullEventBus` for optimization/normal runs (zero overhead)

### 11.3 Phase 2 — JSONL File Sink & Run Identity

**Status: NOT STARTED** | **Depends on:** Phase 1 | **Checklist:** `docs/debug-feature/phase2-checklist.md`

First tangible output — events that pass through the bus get persisted to disk as JSONL.

**Scope (references §4, §3.2):**
- Run folder naming: `{strategy}_v{version}_{asset}_{period}_{params_hash}_{timestamp}` — §4.2
- Directory creation under `Data/EventLogs/` (same AppData root as candle storage) — §4.1
- `JsonlFileSink` implementing `ISink` — append-only writer with `FileShare.Read` — §3.2
- `meta.json` writer populated at `run.end` with config, params, summary stats — §4.1
- Parallel run isolation: each run writes to its own directory — §4.3
- Wire sink registration into DI / backtest setup

### 11.4 Phase 3 — Explicit Event Emission

**Status: NOT STARTED** | **Depends on:** Phase 2 | **Checklist:** `docs/debug-feature/phase3-checklist.md`

Wire `IEventBus.Emit()` calls into all components that produce business-critical events. After this phase, a debug backtest produces a complete chronological event stream.

**Scope (references §10.2, §10.3):**
- Inject `IEventBus` into `BacktestEngine` → emit `bar`, `bar.mut`, `run.start`, `run.end` — §10.2
- Inject into order/execution processing → emit `ord.place`, `ord.fill`, `ord.cancel`, `ord.reject`, `pos` — §10.2
- Inject into `StrategyBase` → emit `sig` — §10.2
- Inject into `IRiskEvaluator` → emit `risk` — §10.2
- Emit `err`, `warn` from engine error paths
- Verify `ExportMode` filtering: optimization runs emit only trade events; backtest emits all

### 11.5 Phase 4 — Indicator Factory & Automatic Emission

**Status: NOT STARTED** | **Depends on:** Phase 3 | **Checklist:** `docs/debug-feature/phase4-checklist.md`

Indicator events emitted automatically via decorator pattern. Zero changes to indicator code — indicators remain pure Domain types.

**Scope (references §10.4–10.9):**
- `IIndicatorFactory` interface in Application — §10.4
- `EmittingIndicator<TInp, TBuff>` generic decorator — §10.5
- `EmittingIndicatorFactory` (wraps with decorator) — §10.6
- `PassthroughIndicatorFactory` (zero overhead for optimization) — §10.6
- Factory selection by run mode at setup time (one-time, no per-call branch) — §10.8
- Inject factory into strategy creation pipeline
- Update existing strategies to use `indicators.Create()` — §10.4
- `ind` / `ind.mut` event emission via decorator — §10.7
- Dual justification: event emission + optimization module composition — §10.9

### 11.6 Phase 5 — Post-Run Persistence (SQLite)

**Status: NOT STARTED** | **Depends on:** Phase 3 | **Checklist:** `docs/debug-feature/phase5-checklist.md`

Build queryable indexes from the JSONL event stream after run completion. Two SQLite databases serve different consumers.

**Scope (references §6):**
- `index.sqlite` builder: parses JSONL → `events` table with `sq`, `ts`, `_t`, `src` indexed columns + `raw` JSON — §6.1
- `trades.sqlite` shared DB: `runs`, `orders`, `trades` tables — §6.2
- Post-run pipeline: JSONL → `index.sqlite` (if enabled in backtest settings), JSONL → `trades.sqlite` (always) — §6.3
- Crash recovery: rebuild from existing JSONL via utility command — §6.1, §6.2
- AI debugger skill file (`SKILL.md`) documenting event schema, file layout, query patterns, debugging workflow decision tree — §8

### 11.7 Phase 6 — WebSocket Transport

**Status: NOT STARTED** | **Depends on:** Phase 2 (sink interface), Phase 0 (probe) | **Checklist:** `docs/debug-feature/phase6-checklist.md`

Replace HTTP POC with real-time bidirectional WebSocket for the visual debugger FE.

**Scope (references §5):**
- WebSocket server hosting in ASP.NET Core — §5.1
- `WebSocketSink` implementing `ISink` — pushes events to connected client — §3.2, §5.1
- Control commands via WebSocket messages (same command set as HTTP POC) — §5.2
- Paused-start mode when visual debug client connects — §5.2
- No-client mode: sink is no-op, engine runs freely — §5.4
- Deprecate/remove HTTP POC endpoints from Phase 0
- One WebSocket connection per observed run

### 11.8 Phase 7 — Per-Event Stepping

**Status: NOT STARTED** | **Depends on:** Phase 1 (event bus), Phase 0 (probe) | **Checklist:** `docs/debug-feature/phase7-checklist.md`

Fine-grained debugging — step by signal, by event type, toggle export categories mid-run.

**Scope (references §5.2, §11.2 original):**
- Extend `IDebugProbe` with `OnEventEmitted(string eventType, DebugSnapshot)`
- New break conditions: `OnSignal`, `OnEventType(string)`, compound AND logic
- New commands: `next_signal`, `next_type { "_t": "..." }`, `set_export { "mutations": true }`
- `GatingDebugProbe` evaluates break conditions against event types (bar-boundary gate becomes a special case)
- `set_export` toggles opt-in event categories (`bar.mut`, `ind.mut`) mid-run via `IEventBus` filter update