# Backtest Debug & Event Export System — Requirements

## 1. Overview

A unified event export system that serves three consumers from a single event stream:

1. **Visual Debugger** — real-time FE client connected via WebSocket, interactive step-through
2. **AI Debugger (Claude Code)** — post-hoc analysis via JSONL files + SQLite index
3. **Trade Logger** — persistent trade/order records, streamed as JSONL during the run, persisted to SQLite after run completes

The system supports multiple execution modes with different export filters, and handles concurrent parallel runs by writing to separate output directories.

---

## 2. Event Model

### 2.1 Canonical Event Shape

All events share a minimal envelope with compact field names:

| Field     | Key  | Type     | Description                                    |
|-----------|------|----------|------------------------------------------------|
| Timestamp | `ts` | ISO 8601 | UTC time of emission                           |
| Sequence  | `sq` | uint64   | Monotonic per-run, guarantees ordering         |
| Type      | `_t` | string   | Event type identifier (see §2.2)               |
| Source    | `src`| string   | Emitting component/subsystem                   |
| Data      | `d`  | object   | Event-specific payload, varies by `_t`         |

Run identity is not carried per-event. It is expressed by the containing directory (see §4).

Each event type is represented by a dedicated C# record type for compile-time safety. FE consumers may use dynamic typing if preferred.

### 2.2 Event Types

**Market data events:**
- `tick` — new tick received
- `bar` — new bar closed
- `bar.mut` — last (open) bar mutated (price update within forming bar)

**Indicator events:**
- `ind` — indicator value computed for a closed bar
- `ind.mut` — indicator value recomputed due to bar mutation

**Signal & risk events:**
- `sig` — signal generated by strategy logic
- `risk` — risk check performed (pass/reject + reason)

**Order lifecycle events:**
- `ord.place` — order submitted
- `ord.fill` — order filled (partial or full)
- `ord.cancel` — order cancelled
- `ord.reject` — order rejected
- `pos` — position changed

**System events:**
- `run.start` — run began (includes config/params snapshot)
- `run.end` — run completed (includes summary stats)
- `err` — error
- `warn` — warning

### 2.3 Export Mode Tags

Each event type is annotated in code with a non-exported `[Flags] enum ExportMode` that determines in which execution contexts the event is emitted:

```
[Flags]
enum ExportMode
{
    Backtest     = 1,
    Optimization = 2,
    Live         = 4,
}
```

Configuration is per event type, defined in code (not runtime config):

| Event type(s)                        | Default ExportMode               | Rationale                                         |
|--------------------------------------|----------------------------------|----------------------------------------------------|
| `ord.*`, `pos`                       | Backtest \| Optimization \| Live | Trade activity is always captured                  |
| `sig`, `risk`                        | Backtest \| Live                 | Decision logic — relevant for debugging, not for mass optimization |
| `bar`, `ind`                         | Backtest                         | Data-level events — debug only                     |
| `tick`                               | Backtest (opt-in)                | Very verbose, only when explicitly needed          |
| `bar.mut`, `ind.mut`                 | Backtest (opt-in)                | Mutation events generate high noise; rarely needed  |
| `run.start`, `run.end`              | Backtest \| Optimization \| Live | Always present as bookends                         |
| `err`, `warn`                        | Backtest \| Optimization \| Live | Always captured                                    |

The event bus checks `eventType.ExportMode.HasFlag(currentRunMode)` before serialization. Events that don't match are silently dropped — never serialized, never written to any sink.

### 2.4 Data Subscription Export Control

Bar and indicator export is governed by the `DataSubscription` configuration:

```csharp
public record DataSubscription(Asset Asset, TimeSpan TimeFrame, bool IsExportable = false);
```

A strategy declares one or more data subscriptions (e.g. ETH-USD M1, ETH-USD H1, BTC-USD H4). Only subscriptions with `IsExportable = true` have their `bar`, `bar.mut`, `ind`, and `ind.mut` events emitted to sinks. Non-exportable subscriptions are still processed internally by the strategy but produce no event output.

This allows fine-grained control in multi-asset, multi-timeframe strategies. For example, a strategy consuming M1 bars for signal calculation but operating on H1 decisions would mark only the H1 subscription as exportable — avoiding noise from 60x more M1 bar events.

The visual debugger draws and steps through bars from exportable subscriptions only. Post-run reports likewise use exportable subscriptions for charting.

---

## 3. Event Bus Architecture

### 3.1 Single Emission Point

Strategy code, indicators, execution engine, and risk manager all emit events through a single `IEventBus` abstraction. The bus:

- Checks `ExportMode` tag against the current run mode — drops non-matching events
- Applies `DataSubscription.IsExportable` filter for bar/indicator events — only emits from exportable subscriptions
- Serializes each surviving event to JSON **once**
- Fans out the serialized payload to all registered sinks

### 3.2 Sinks

| Sink | Purpose | Receives |
|------|---------|----------|
| **JSONL File Sink** | Post-hoc AI/human analysis | All exported events (after mode + TF filtering) |
| **WebSocket Sink** | Real-time FE visual debugger | All exported events (after mode + TF filtering) |

Trade logging is not a separate sink — trade events (`ord.*`, `pos`) flow through the same JSONL file sink as all other events. They are extracted into the Trade DB during post-run index build (see §6).

---

## 4. File Organization & Run Identity

The following structure should be in the same AlgoTradeForge dir (..AppData/localAlgoTradeForge on Windows) that contains partitioned Candle data from CandleIngestor.

### 4.1 Directory Structure

```
Data/
  EventLogs/
    {run_folder_name}/
      events.jsonl        # complete chronological event stream
      meta.json           # run config, params, summary (written at run.end)
      index.sqlite        # AI debug query index (built post-run, if enabled)
  trades.sqlite           # persistent trade DB, shared across all runs
```

### 4.2 Run Folder Naming

Run identity is encoded in the folder name, not in individual events. The folder name is a human-readable identifier:

```
{strategy_name}_v{version}_{asset}_{period}_{params_hash}_{timestamp}
```

Examples:
```
MeanRevert_v2.3_ETH-USD_H1_2024-2025_a3f8c1_20260208T143201
TrendFollow_v1.0_BTC-USD_M15_2025-2026_b7d2e4_20260208T150000
```

Components:
- `strategy_name` + `version` — human-readable identification
- `asset` + `period` — the primary backtest parameters
- `params_hash` — short hash of the full parameter set (for uniqueness when same strategy/asset runs with different params)
- `timestamp` — run start time (prevents collision on re-runs)

The full parameter set is stored inside `meta.json`, not in the folder name.

### 4.3 Parallel Run Isolation

Each concurrent run (e.g. during optimization) writes to its own directory. There is no shared file or stream between runs. Isolation is structural (separate directories), not logical (filtering by run ID within a shared stream).

---

## 5. WebSocket Interface (Visual Debugger)

### 5.1 Connection Model

The backtest runner hosts a WebSocket server. The FE client connects to a specific run's channel.

- One WebSocket connection per observed run
- Events are pushed to the client as they are emitted (same JSON as file sink)
- If no FE client is connected, the sink is a no-op (events still go to file)

### 5.2 Execution Control via WebSocket

The FE client sends **control messages** to govern execution flow. The runner starts in **paused** state when a visual debug session is active.

**Control commands:**

| Command | Payload | Behavior |
|---------|---------|----------|
| `continue` | — | Run to completion without pausing |
| `next` | — | Advance to the next exported event (any type), then pause |
| `next_type` | `{ "_t": "bar" }` | Advance until the next event of the given type is exported, then pause |
| `next_bar` | — | Shortcut: advance to next `bar` event from any exportable subscription, then pause |
| `next_signal` | — | Shortcut: advance to next `sig` event, then pause |
| `next_trade` | — | Advance until next order lifecycle event, then pause |
| `run_to` | `{ "sq": 5000 }` or `{ "ts": "..." }` | Run until a specific sequence number or timestamp, then pause |
| `set_export` | `{ "mutations": true }` | Toggle opt-in event categories mid-run (e.g. enable `bar.mut`, `ind.mut`) |
| `pause` | — | Pause execution after the current event completes |

All stepping commands operate on **exported** events only (post-filter). `next_bar` steps to the next bar from any exportable `DataSubscription`, skipping bars from non-exportable subscriptions.

### 5.3 Pause Semantics

When paused, the runner:

- Has fully processed and emitted the current event
- Holds all state (open positions, indicator buffers, current bar) in memory
- Waits for the next control command before advancing
- Remains responsive to WebSocket control messages

When `continue` is issued, the runner stops pausing between events and runs at full speed, still emitting events to all sinks in real-time.

### 5.4 No-Client Mode

When running without a visual debugger (normal backtest or optimization), the WebSocket server is either not started or execution is in `continue` mode by default (no pausing). Determined by run configuration.

---

## 6. Post-Run Persistence

Both persistent stores are built from the JSONL event stream after the run completes or is interrupted. During the run, only JSONL is written (plus WebSocket emission if active).

### 6.1 AI Debug Index (index.sqlite)

**Trigger:** built post-run **if enabled in backtest settings**. Not built for optimization runs by default.

**Source:** parses the run's `events.jsonl`.

**Schema:**
- `events` table — all events with extracted top-level fields (`sq`, `ts`, `_t`, `src`) as indexed columns + full JSON in a `raw` column
- Indexed on: `sq`, `ts`, `_t`, `src`

**Location:** `Data/EventLogs/{run_folder}/index.sqlite` — co-located with the JSONL it indexes.

**Crash resilience:** if the run terminates abnormally, the JSONL file is the only artifact. The index can be rebuilt later manually or by a utility command operating on the existing JSONL.

### 6.2 Trade DB (trades.sqlite)

**Trigger:** built post-run **always**, for every execution mode (backtest, optimization, live).

**Source:** extracts `ord.*` and `pos` events from the run's `events.jsonl`.

**Schema:**
- `runs` table — run folder name, strategy, version, asset, period, full params (JSON), start/end time, mode, summary stats
- `orders` table — FK to run, full order lifecycle
- `trades` table — FK to order, individual fills

**Location:** `Data/trades.sqlite` — single shared file across all runs.

**Write behavior:** inserts transactionally after the run completes. For optimization with many parallel runs, each run's trade data is inserted in a single transaction upon that run's completion.

**Crash resilience:** if the run terminates abnormally, trade data can still be recovered by re-extracting from the JSONL (same as index build).

### 6.3 Lifecycle Summary

```
During run:      events.jsonl ← append (+ WebSocket push if visual debug)
                 meta.json ← written at run.end

After run:       events.jsonl → index.sqlite    (if enabled)
                 events.jsonl → trades.sqlite   (always)
```

---

## 7. Execution Modes Summary

| Mode | JSONL Export | AI Debug Index | Trade DB | WebSocket | Default Export |
|------|-------------|----------------|----------|-----------|----------------|
| **Debug (visual)** | ✓ | ✓ | ✓ | ✓ (paused start) | `Backtest` flag events |
| **Debug (AI/headless)** | ✓ | ✓ | ✓ | ✗ | `Backtest` flag events |
| **Backtest** | ✓ | Optional | ✓ | ✗ | `Backtest` flag events |
| **Optimization** | ✓ (trades only) | ✗ | ✓ | ✗ | `Optimization` flag events |
| **Prod / Paper** | ✓ | ✗ | ✓ | ✗ | `Live` flag events |

---

## 8. AI Debugger Experience (Claude Code)

### 8.1 Available Artifacts Per Run

Claude Code has access to:

1. `meta.json` — read first, understand what the run was
2. `events.jsonl` — sequential reading for narrative debugging
3. `index.sqlite` — SQL queries for analytical debugging (if built)
4. Skill file documenting the event schema, file layout, and recommended workflows

### 8.2 Expected Debugging Workflows

- **Triage:** read `meta.json` summary → query `index.sqlite` for error/warning count → read those events from JSONL for context
- **Execution debugging:** query orders from `index.sqlite` → find sequence range → read surrounding events from JSONL to see what led to the order
- **Indicator debugging:** query specific indicator values from `index.sqlite` → compare expected vs actual → read raw bar data around anomalies
- **Comparison across runs:** query `index.sqlite` from two runs → diff trade outcomes → investigate divergence points in JSONL

### 8.3 Skill File Contract

A skill file (`SKILL.md`) is provided that documents:
- Event envelope schema and compact field names
- All event types and their `d` (data) payload shapes
- File layout and naming conventions
- Recommended `jq` and `sqlite3` query patterns
- Debugging workflow decision tree (start here → then check this → then look at that)

---

## 9. Design Decisions Log

| # | Decision | Rationale |
|---|----------|-----------|
| 1 | Flag enum `ExportMode` over logging levels | Export is per-event-type configuration, not a severity hierarchy. A bar event isn't "less important" than a trade — it's relevant in different contexts. |
| 2 | `DataSubscription.IsExportable` flag over global reporting timeframe | Per-subscription export control supports multi-asset, multi-TF strategies naturally. No single "reporting TF" assumption — each subscription decides independently. |
| 3 | No run ID in event payload | Run identity is structural (directory), not per-event metadata. Eliminates redundant bytes and simplifies the event model. |
| 4 | Single chronological JSONL, no partitioning | Preserves narrative flow for LLM sequential reasoning. SQLite index provides type-based access post-hoc. |
| 5 | JSONL as source of truth, SQLite as derived | Simplifies the write path (append-only file). Both indexes are built from the same source after run completion. |
| 6 | Trade DB shared across runs | Enables cross-run queries (all ETH-USD trades across strategy versions) without opening N directories. |
| 7 | Strictly typed C# records per event type | Compile-time safety for event payloads. FE may use dynamic typing independently. |
| 8 | Hybrid emission: explicit `IEventBus` injection + factory-decorated indicators | Keeps business-critical emissions visible in code while eliminating boilerplate from pure-computation indicators. Factory earns dual justification: event emission control and centralized indicator construction for optimization sweeps. See §10. |
| 9 | `IIndicatorFactory` + generic decorator over source-generated proxies | Strategies create indicators directly and hold concrete/interface references. A factory provides the interception point for wrapping without engine needing visibility into strategy internals. One hand-written generic decorator covers all indicator types — no source generation complexity. See §10.4–10.6. |

---

## 10. Event Emission API — Hybrid Approach

### 10.1 Design Goal

Event emission is a cross-cutting concern that touches strategy code, the execution engine, risk management, and indicators. The API must be:

- **Lightweight** — minimal ceremony at the call site
- **Explicit where it matters** — business-critical events (signals, orders, risk decisions) should be visible in the code that produces them
- **Invisible where it doesn't** — indicators are pure computations; they should not know events exist

### 10.2 Two Emission Mechanisms

| Emission point | Mechanism | Rationale |
|---|---|---|
| Strategy signals (`sig`) | Explicit `IEventBus.Emit()` call | Signals are business decisions — the developer should see the emission in code |
| Risk checks (`risk`) | Explicit `IEventBus.Emit()` call | Same — these are judgment calls, not boilerplate |
| Order lifecycle (`ord.*`, `pos`) | Explicit `IEventBus.Emit()` call | The execution engine explicitly decides when orders change state |
| Bar events (`bar`, `bar.mut`) | Explicit `IEventBus.Emit()` call | The backtest engine owns the bar loop — natural single emission point |
| System events (`run.*`, `err`, `warn`) | Explicit `IEventBus.Emit()` call | Infrastructure-level, emitted by the engine at well-defined lifecycle points |
| Indicator values (`ind`, `ind.mut`) | **Automatic via factory-decorated indicator** | Indicators follow a uniform `Compute(series) → values` contract; emission is mechanical and identical for every indicator. A factory wraps indicators with a generic decorator at creation time. |

### 10.3 Explicit Injection Path

Components that emit business-critical events receive `IEventBus` via standard constructor injection:

```csharp
public interface IEventBus
{
    void Emit<T>(T evt) where T : IBacktestEvent;
}
```

The bus implementation handles all filtering internally (`ExportMode` check, `DataSubscription.IsExportable` check). Call sites never check filters — they emit unconditionally and the bus drops what doesn't match. This keeps call sites to a single line.

The classes that receive `IEventBus` are few and well-defined: the backtest engine, the execution engine, the risk manager, and the strategy base class. This is not a "sprinkled everywhere" dependency.

### 10.4 Indicator Factory Path

Strategies create indicators through an injected `IIndicatorFactory`. The factory is the interception point for event emission — it wraps indicators with a generic decorator that emits `ind` events after each `Compute()` call.

```csharp
public interface IIndicatorFactory
{
    IIndicator<TInp, TBuff> Create<TInp, TBuff>(
        IIndicator<TInp, TBuff> indicator,
        DataSubscription subscription);
}
```

Strategy code creates indicators by passing a raw instance and the associated subscription:

```csharp
public sealed class ZigZagBreakoutStrategy(
    ZigZagBreakoutParams parameters,
    IIndicatorFactory indicators) : StrategyBase<ZigZagBreakoutParams>(parameters)
{
    private IIndicator<Int64Bar, long> _dzz = null!;

    public override void OnInit()
    {
        _dzz = indicators.Create(
            new DeltaZigZag(Params.DzzDepth / 10m, Params.MinimumThreshold),
            DataSubscriptions[0]);
    }

    public override void OnBar(Int64Bar bar, DataSubscription subscription, IOrderContext orders)
    {
        _barHistory.Add(bar);
        _dzz.Compute(_barHistory);          // decorator emits ind event here
        var values = _dzz.Buffers["Value"];  // works through interface
        // ... strategy logic unchanged
    }
}
```

The strategy holds an `IIndicator<TInp, TBuff>` interface reference. All existing indicator interface members (`Buffers`, `Name`, `Measure`, `Compute`) are accessible through the interface — no concrete type access is lost for strategy logic.

**Key property:** indicators remain pure Domain types with zero dependencies on the event system. They can be unit-tested in complete isolation. The emission concern is handled entirely by the factory and decorator infrastructure.

### 10.5 Generic Decorator

A single hand-written generic decorator class covers all indicator types — no source generation required:

```csharp
internal sealed class EmittingIndicator<TInp, TBuff>(
    IIndicator<TInp, TBuff> inner,
    IEventBus bus,
    DataSubscription sub) : IIndicator<TInp, TBuff>
{
    public string Name => inner.Name;
    public IndicatorMeasure Measure => inner.Measure;
    public IReadOnlyDictionary<string, IReadOnlyList<TBuff>> Buffers => inner.Buffers;
    public int MinimumHistory => inner.MinimumHistory;
    public int? CapacityLimit => inner.CapacityLimit;

    public void Compute(IReadOnlyList<TInp> series)
    {
        inner.Compute(series);
        bus.Emit(new IndEvent(inner.Name, inner.Measure, /* latest buffer values */, sub));
    }
}
```

The decorator is strongly typed — no reflection, no boxing of `Int64Bar` (a `record struct`), no allocation beyond the event record itself. Performance is identical to hand-written delegation code.

**Why not source generation:** The original design considered a Roslyn source generator to auto-produce decorator classes per indicator interface. This was rejected because:

1. **One generic class suffices** — `IIndicator<TInp, TBuff>` is the only indicator interface. A single `EmittingIndicator<TInp, TBuff>` covers all concrete indicators without code generation.
2. **No interception point existed** — source-gen proxies assumed the engine would wrap indicators at setup time, but strategies create indicators internally. The factory pattern provides this interception point explicitly, making the source generator's wrapping mechanism redundant.
3. **Build complexity** — source generators add a project dependency, complicate debugging, and require maintenance of generator code. A plain class in the infrastructure layer is simpler.

**Why not `DispatchProxy`:** .NET's runtime proxy mechanism (`System.Reflection.DispatchProxy`) suffers from reflection overhead, argument boxing (`Int64Bar` is a `record struct` → heap allocation per call), and return value boxing. For 500K+ bars × 3–5 indicators, this creates millions of unnecessary allocations. The hand-written generic decorator avoids all three.

### 10.6 Factory Implementations

Two factory implementations, selected at run setup time:

```csharp
// Backtest / debug — wraps with emission decorator
internal sealed class EmittingIndicatorFactory(IEventBus bus) : IIndicatorFactory
{
    public IIndicator<TInp, TBuff> Create<TInp, TBuff>(
        IIndicator<TInp, TBuff> indicator, DataSubscription sub)
        => new EmittingIndicator<TInp, TBuff>(indicator, bus, sub);
}

// Optimization — pass-through, zero overhead
internal sealed class PassthroughIndicatorFactory : IIndicatorFactory
{
    public IIndicator<TInp, TBuff> Create<TInp, TBuff>(
        IIndicator<TInp, TBuff> indicator, DataSubscription sub)
        => indicator;
}
```

The engine selects the factory at run setup based on execution mode. This is a one-time decision — no per-call branches.

### 10.7 Multi-Subscription Correctness

In multi-asset or multi-timeframe strategies (e.g. ETH M1 + ETH H1 with different indicators per timeframe), indicator event emission must fire only when an indicator actually computes — not on every `OnBar` call.

The decorator achieves this by construction: the `ind` event is emitted inside `Compute()`, so it fires if and only if the strategy calls `Compute` on that indicator for the current bar. If the strategy only computes the H1 indicator on H1 bars, only H1 bars produce `ind` events for that indicator.

This is a correctness advantage over alternative approaches (e.g. emitting all tracked indicators from `StrategyBase.OnBar`) which would require manual association of indicators to subscriptions to avoid emitting stale snapshots.

### 10.8 Opt-Out for Optimization Runs

In optimization mode, `ind` events are not in the `ExportMode` filter — the bus would drop them. But the overhead of *creating* the event record before the bus drops it is still wasteful at scale.

The factory approach provides clean opt-out: the engine injects `PassthroughIndicatorFactory` for optimization runs. The factory returns the raw indicator unwrapped — zero overhead, no allocation, no flag check per bar. This is a setup-time decision, not a per-call branch.

### 10.9 Dual Justification for the Factory

The `IIndicatorFactory` abstraction earns its existence for two independent reasons:

1. **Event emission control** — wrapping/not-wrapping indicators with the emitting decorator based on execution mode.
2. **Optimization module composition** — the optimization framework (see `optimizeable_params_framework.md` §7) supports indicators as pluggable `[OptimizableModule]` slots on strategy params. When the optimization runner resolves a module variant (e.g., SMA with Period=20 vs EMA with Period=20, Smoothing=0.2), it creates a raw `IIndicator<TInp, TBuff>` instance and injects it into the strategy params. The strategy then passes it through `IIndicatorFactory.Create()` — the same code path as manually constructed indicators. This means `IIndicatorFactory` is the **universal decoration point** regardless of how the indicator was created:

```
Manual construction:   new DeltaZigZag(...)  ──→ IIndicatorFactory.Create() ──→ wrapped/passthrough
Module injection:      registry.Create(...)  ──→ IIndicatorFactory.Create() ──→ wrapped/passthrough
```

The two features compose without coupling: the optimization framework owns indicator **selection and construction**, while `IIndicatorFactory` owns **decoration for event emission**. Neither knows about the other.

---

## 11. Implementation Status

### 11.1 MVP — Debug Probe (Control Loop Gating)

**Status: DONE** (branch `007-debug-control-in-loop`)

The MVP implements execution control via an injected `IDebugProbe` that gates the `BacktestEngine.Run()` loop at **bar boundaries**. This is a POC of the debug probe concept — event serialization (JSONL, IEventBus) and WebSocket transport are out of scope for this phase.

**Architecture**: See `docs/debug-feature-control-loop-plan.md` for the full design.

**MVP commands implemented:**

| Command | Break Condition | Description | Status |
|---------|----------------|-------------|--------|
| `next_bar` | `Always` | Step to the next bar (any subscription) | **Done** |
| `next` | `OnExportableBar` | Step to the next bar from an exportable subscription | **Done** |
| `next_trade` | `OnFillBar` | Step to the next bar that produces fills | **Done** |
| `run_to_sequence` | `AtSequence(n)` | Run until sequence number >= n | **Done** |
| `run_to_timestamp` | `AtTimestamp(ms)` | Run until bar timestamp >= ms | **Done** |
| `continue` | `Never` | Run to completion without pausing | **Done** |
| `pause` | `Always` | Pause at the next bar boundary | **Done** |

These commands cover the core debugging scenarios: stepping bar-by-bar, jumping to a specific point in time, and finding the next trade.

**HTTP endpoints (temporary POC — will be replaced by WebSocket transport in §5):**

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/api/debug-sessions/` | Start a debug session (engine starts paused) |
| `POST` | `/api/debug-sessions/{id}/commands` | Send a control command, returns `DebugSnapshot` |
| `GET` | `/api/debug-sessions/{id}` | Get session status + last snapshot |
| `DELETE` | `/api/debug-sessions/{id}` | Terminate and clean up session |

**Files added/modified:**

| File | Layer | Change |
|------|-------|--------|
| `IDebugProbe.cs` | Domain | Interface: `IsActive`, `OnRunStart`, `OnBarProcessed`, `OnRunEnd` |
| `DebugSnapshot.cs` | Domain | `readonly record struct` with sequence, timestamp, subscription index, exportable flag, fill count, equity |
| `NullDebugProbe.cs` | Domain | Singleton null-object (`IsActive = false`) for non-debug runs |
| `BacktestEngine.cs` | Domain | Added optional `IDebugProbe?` param + 3 guarded call sites |
| `BacktestOrderContext.cs` | Domain | Extracted from engine for fill tracking |
| `DebugCommand.cs` | Application | Sealed record hierarchy for all commands |
| `BreakCondition.cs` | Application | Abstract record with `ShouldBreak(DebugSnapshot)` |
| `GatingDebugProbe.cs` | Application | `ManualResetEventSlim`-based gate, `SendCommandAsync` for HTTP thread |
| `DebugSession.cs` | Application | Session object owning probe + background `Task` |
| `IDebugSessionStore.cs` | Application | Session store interface |
| `InMemoryDebugSessionStore.cs` | Application | `ConcurrentDictionary`-based store |
| `DebugSessionDto.cs` | Application | DTOs for HTTP responses |
| `StartDebugSessionCommand[Handler].cs` | Application | Creates session, launches engine on `LongRunning` thread |
| `SendDebugCommand[Handler].cs` | Application | Sends command to probe, returns snapshot |
| `DebugContracts.cs` | WebApi | HTTP request records |
| `DebugEndpoints.cs` | WebApi | REST endpoints (temporary POC) |

### 11.2 Future — Per-Event Stepping (requires IEventBus)

**Status: NOT STARTED** — depends on §3 (Event Bus Architecture) and §10 (Event Emission API).

Once `IEventBus` is implemented, the debug probe can observe individual event types (signals, indicators, order placements) rather than only bar boundaries. This enables:

| Command | Description | Prerequisite |
|---------|-------------|-------------|
| `next_signal` | Advance to next `sig` event | IEventBus emitting signal events |
| `next_type { "_t": "..." }` | Advance to next event of any given type | IEventBus with event type metadata |
| `set_export` | Toggle opt-in event categories mid-run | IEventBus + ExportMode filtering |

**Extension path**: Add `OnEventEmitted(string eventType, DebugSnapshot)` to `IDebugProbe`. The `GatingDebugProbe` evaluates break conditions against event type in addition to bar-level counters. The bar-boundary gate (`OnBarProcessed`) becomes a special case.

**Compound conditions** (e.g., "next trade after timestamp X") can be supported by composing existing `BreakCondition` subclasses with AND logic — the `BreakCondition` abstraction already supports this pattern.

### 11.3 Future — WebSocket Transport

**Status: NOT STARTED** — depends on §5 (WebSocket Interface).

The current HTTP REST endpoints are a temporary POC for validating the debug probe mechanism. The production transport will use WebSocket connections per §5.1, with the same command set delivered as WebSocket messages instead of HTTP requests. The `GatingDebugProbe` and `BreakCondition` infrastructure remains unchanged — only the transport layer is replaced.